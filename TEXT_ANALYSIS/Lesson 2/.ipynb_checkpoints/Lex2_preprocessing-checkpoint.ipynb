{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0324a55",
   "metadata": {},
   "source": [
    "# Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9629f",
   "metadata": {},
   "source": [
    "### How to install packages in Anaconda\n",
    "\n",
    "https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#viewing-a-list-of-installed-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ec108",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03984693",
   "metadata": {},
   "source": [
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd6d07",
   "metadata": {},
   "source": [
    "To install it use:\n",
    "\n",
    "!pip install nltk\n",
    "\n",
    "or \n",
    "\n",
    "conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644251d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')\n",
    "'''\n",
    "\n",
    "from nltk.book import text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05165985",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c884c2",
   "metadata": {},
   "source": [
    "### Playing with Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418391d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5745e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dcf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa499adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_raw[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf479d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_tokens[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b97f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to work with the novel in nltk.Text format 'text1' variable\n",
    "\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab137c4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # How many tokens (words and punctuation symbols) are in text1?\n",
    "\n",
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84199bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique tokens (unique words and punctuation) does text1 have?\n",
    "\n",
    "len(set(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After lemmatizing ONLY the verbs, how many unique tokens does text1 have?\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "len(set(lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a437c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical diversity of the text = ratio of unique tokens to the total number of tokens\n",
    "\n",
    "len(set(text1))/len(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9837c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(text1.vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d93f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.vocab().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637699fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting a single word occurences: percentage of tokens that are 'whale'or 'Whale'?\n",
    "\n",
    "val = (text1.vocab()['whale'] + text1.vocab()['Whale']) / len(text1)\n",
    "\n",
    "format(val, \".3%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb742930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "help(operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb004e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "sorted(text1.vocab().items(), \n",
    "       key=operator.itemgetter(1), \n",
    "       reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "sorted( [token for token, freq in text1.vocab().items()\n",
    "        if len(token) > 5 and freq > 150] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4facaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the longest word in text1 and that word's length\n",
    "\n",
    "sorted([(token, len(token)) for token, freq in text1.vocab().items()], \n",
    "       key=operator.itemgetter(1), reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"string\"\n",
    "help(s.isalpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0803148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\",\".isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e36f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What unique words (no punctuations!) have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "sorted([(freq, token) for token, freq in text1.vocab().items() if freq > 2000 and token.isalpha()], \n",
    "       key=operator.itemgetter(0), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1942ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average number of tokens per sentence\n",
    "\n",
    "np.mean([len(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(moby_raw)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6328e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 most frequent parts of speech in this text\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "sorted(Counter([tag for token, tag in nltk.pos_tag(text1)]).items(), \n",
    "       key=operator.itemgetter(1), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our example: first sentence from the book Pride and Prejudice as the text\n",
    "\n",
    "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation\n",
    "\n",
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "print(text_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e17fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(text_p)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a27010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword Filtering\n",
    "\n",
    "## we can use nltk.corpus.stopwords.words('english') to fetch a list of stopwords in the English dictionary\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## let's see them\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1366e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "## we stem the tokens using nltk.stem.porter.PorterStemmer to get the stemmed tokens\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmed = [porter.stem(word) for word in filtered_words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8143409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS (part-of-speech) Tagger\n",
    "\n",
    "## we can use nltk.pos_tag to retrieve the part of speech of each token in a list\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos = pos_tag(filtered_words)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-# FULL PREPROCESSING\n",
    "# We can combine all the preprocessing methods above and create a preprocess function that takes in a .txt file and handles all the preprocessing\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "def preprocess(filename):\n",
    "    f = open(filename,'r')\n",
    "    text = f.read()\n",
    "    text = text.lower()\n",
    "    \n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    words = word_tokenize(text_p)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in filtered_words]\n",
    "    \n",
    "    pos = pos_tag(filtered_words)\n",
    "    \n",
    "    return words, filtered_words, stemmed, pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, filtered_words, stemmed, pos = preprocess('pride_and_prejudice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b545c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words:', words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabdd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filtered words:', filtered_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stemmed words:', stemmed[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Part of Speech:', pos[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3acd216",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "\n",
    "myList = [1,2,3,4,5]\n",
    "\n",
    "list(bigrams(myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d26f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import trigrams\n",
    "\n",
    "print(trigrams(myList))\n",
    "\n",
    "list(trigrams(myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "\n",
    "sent = 'a b c'.split()\n",
    "\n",
    "list(everygrams(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1aac53",
   "metadata": {},
   "source": [
    "### the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
