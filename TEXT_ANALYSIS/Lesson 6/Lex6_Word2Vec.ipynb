{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08510f06",
   "metadata": {},
   "source": [
    "# Lesson 6\n",
    "\n",
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261d4f3",
   "metadata": {},
   "source": [
    "The dataset we'll be using: simpsons_dataset.csv\n",
    "\n",
    "https://www.kaggle.com/pierremegret/dialogue-lines-of-the-simpsons\n",
    "\n",
    "\n",
    "We'll use only two columns:\n",
    "\n",
    "- raw_character_text: the character who speaks (can be useful when monitoring the preprocessing steps)\n",
    "- spoken_words: the raw text from the line of dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a5693",
   "metadata": {},
   "source": [
    "To install spacy copy and run one of these codelines:\n",
    "\n",
    "!pip install spacy\n",
    "\n",
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126711cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                             # For preprocessing\n",
    "import pandas as pd                   # For data handling\n",
    "from time import time                 # To time our operations\n",
    "from collections import defaultdict   # For word frequency\n",
    "\n",
    "import spacy                          # For preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7463c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the loggings to monitor gensim\n",
    "\n",
    "import logging                        \n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", \n",
    "                    datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bd18e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747894a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import csv file\n",
    "\n",
    "df = pd.read_csv('simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4491e72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e724fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c68a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    0\n",
       "spoken_words          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2697f74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131853"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9780a30",
   "metadata": {},
   "source": [
    "Lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26870995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in w:\\programmi\\anaconda\\lib\\site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.61.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in w:\\programmi\\anaconda\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in w:\\programmi\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in w:\\programmi\\anaconda\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in w:\\programmi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in w:\\programmi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in w:\\programmi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in w:\\programmi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.5)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in w:\\programmi\\anaconda\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in w:\\programmi\\anaconda\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.0.0\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9ea108",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff03801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes non-alphabetic characters\n",
    "\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', \n",
    "                         str(row)).lower() for row in df['spoken_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a7b4211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 2.02 mins\n"
     ]
    }
   ],
   "source": [
    "# Take advantage of spaCy .pipe() attribute to speed-up the cleaning process\n",
    "\n",
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5215721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method pipe in module spacy.language:\n",
      "\n",
      "pipe(texts: Iterable[str], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) method of spacy.lang.en.English instance\n",
      "    Process texts as a stream, and yield `Doc` objects in order.\n",
      "    \n",
      "    texts (Iterable[str]): A sequence of texts to process.\n",
      "    as_tuples (bool): If set to True, inputs should be a sequence of\n",
      "        (text, context) tuples. Output will then be a sequence of\n",
      "        (doc, context) tuples. Defaults to False.\n",
      "    batch_size (Optional[int]): The number of texts to buffer.\n",
      "    disable (List[str]): Names of the pipeline components to disable.\n",
      "    component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      "        arguments for specific components.\n",
      "    n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      "    YIELDS (Doc): Documents in the order of the original text.\n",
      "    \n",
      "    DOCS: https://spacy.io/api/language#pipe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp.pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b025f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85956, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the results in a DataFrame to remove missing values and duplicates ALWAYS REMOVE DUPLICATES\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20396590",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82e04a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "008ff317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrases() takes a list of list of words as input\n",
    "\n",
    "sent = [row.split() for row in df_clean['clean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21d19497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:39:45: collecting all words and their counts\n",
      "INFO - 19:39:45: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 19:39:45: PROGRESS: at sentence #10000, processed 63557 words and 52733 word types\n",
      "INFO - 19:39:46: PROGRESS: at sentence #20000, processed 130938 words and 99702 word types\n",
      "INFO - 19:39:46: PROGRESS: at sentence #30000, processed 192959 words and 138314 word types\n",
      "INFO - 19:39:46: PROGRESS: at sentence #40000, processed 249828 words and 172378 word types\n",
      "INFO - 19:39:46: PROGRESS: at sentence #50000, processed 311267 words and 208202 word types\n",
      "INFO - 19:39:46: PROGRESS: at sentence #60000, processed 373573 words and 243255 word types\n",
      "INFO - 19:39:46: PROGRESS: at sentence #70000, processed 436422 words and 278194 word types\n",
      "INFO - 19:39:46: PROGRESS: at sentence #80000, processed 497885 words and 311308 word types\n",
      "INFO - 19:39:46: collected 330094 token types (unigram + bigrams) from a corpus of 537096 words and 85956 sentences\n",
      "INFO - 19:39:46: merged Phrases<330094 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 19:39:46: Phrases lifecycle event {'msg': 'built Phrases<330094 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.90s', 'datetime': '2021-06-24T19:39:46.758095', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Creates the relevant phrases from the list of sentences\n",
    "\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b97cf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:39:47: exporting phrases from Phrases<330094 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 19:39:48: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<124 phrases, min_count=30, threshold=10.0> from Phrases<330094 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.84s', 'datetime': '2021-06-24T19:39:48.099830', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4742879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the corpus based on the bigrams detected\n",
    "\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ace1b6b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformedCorpus' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-2b0b2e3fb19f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TransformedCorpus' object has no attribute 'text'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83b63705",
   "metadata": {},
   "source": [
    "**Most Frequent Words**\n",
    "\n",
    "Mainly a sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39d94d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29493"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "483fa109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'like', 'know', 'get', 'hey', 'think', 'right', 'look', 'want', 'come']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f48703",
   "metadata": {},
   "source": [
    "### Gensim Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2173b42",
   "metadata": {},
   "source": [
    "Separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "\n",
    "- Word2Vec():\n",
    "In this first step, set up the parameters of the model one-by-one.\n",
    "We do not supply the parameter sentences, and therefore leave the model uninitialized, purposefully.\n",
    "\n",
    "- .build_vocab():\n",
    "Here it builds the vocabulary from a sequence of sentences and thus initialized the model.\n",
    "With the loggings, we can follow the progress and even more important, the effect of min_count and sample on the word corpus. It was noticed that these two parameters, and in particular sample, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "\n",
    "- .train():\n",
    "Finally, trains the model.\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b678a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9849a668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "print(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be11eb2",
   "metadata": {},
   "source": [
    "The parameters:\n",
    "- min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "- window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "- size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "- sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "- alpha = float - The initial learning rate - (0.01, 0.05)\n",
    "- min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "- negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "- workers = int - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fd4fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:42:39: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2021-06-24T19:42:39.747541', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=100,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f1a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.utils.SaveLoad)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None)\n",
      " |  \n",
      " |  Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
      " |  \n",
      " |  Warnings\n",
      " |  --------\n",
      " |  This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
      " |  such as lambda functions etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None)\n",
      " |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |      \n",
      " |      Once you're finished training a model (=no more updates, only querying)\n",
      " |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      " |      to reduce memory.\n",
      " |      \n",
      " |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |      \n",
      " |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupr√©, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, rethrow=False, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b3f61",
   "metadata": {},
   "source": [
    "**Building the Vocabulary Table**\n",
    "\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d22295c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:42:59: collecting all words and their counts\n",
      "INFO - 19:42:59: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #10000, processed 61705 words, keeping 9474 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #20000, processed 127321 words, keeping 14329 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #30000, processed 187814 words, keeping 17358 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #40000, processed 243317 words, keeping 20021 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #50000, processed 303178 words, keeping 22426 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #60000, processed 363915 words, keeping 24662 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #70000, processed 425375 words, keeping 26806 word types\n",
      "INFO - 19:43:00: PROGRESS: at sentence #80000, processed 485511 words, keeping 28619 word types\n",
      "INFO - 19:43:00: collected 29493 word types from a corpus of 523625 raw words and 85956 sentences\n",
      "INFO - 19:43:00: Creating a fresh vocabulary\n",
      "INFO - 19:43:00: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 3316 unique words (11.243345878683078%% of original 29493, drops 26177)', 'datetime': '2021-06-24T19:43:00.943762', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 19:43:00: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 438138 word corpus (83.67400334208642%% of original 523625, drops 85487)', 'datetime': '2021-06-24T19:43:00.944761', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 19:43:00: deleting the raw counts dictionary of 29493 items\n",
      "INFO - 19:43:00: sample=6e-05 downsamples 1204 most-common words\n",
      "INFO - 19:43:00: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 199589.18938607664 word corpus (45.6%% of prior 438138)', 'datetime': '2021-06-24T19:43:00.978669', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 19:43:01: estimated required memory for 3316 words and 100 dimensions: 4310800 bytes\n",
      "INFO - 19:43:01: resetting layer weights\n",
      "INFO - 19:43:01: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-06-24T19:43:01.019561', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe3660",
   "metadata": {},
   "source": [
    "**Training of the model**\n",
    "\n",
    "\n",
    "Parameters of the training:\n",
    "\n",
    "- total_examples = int - Count of sentences;\n",
    "- epochs = int - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afbe9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:43:06: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 3316 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2', 'datetime': '2021-06-24T19:43:06.236482', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO - 19:43:07: EPOCH 1 - PROGRESS: at 80.38% examples, 159924 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 19:43:07: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:07: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:07: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:07: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:07: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:07: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:07: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:07: EPOCH - 1 : training on 523625 raw words (199374 effective words) took 1.1s, 188875 effective words/s\n",
      "INFO - 19:43:08: EPOCH 2 - PROGRESS: at 53.71% examples, 105452 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 19:43:08: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:08: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:08: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:08: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:08: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:08: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:08: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:08: EPOCH - 2 : training on 523625 raw words (199392 effective words) took 1.2s, 163504 effective words/s\n",
      "INFO - 19:43:09: EPOCH 3 - PROGRESS: at 51.81% examples, 98175 words/s, in_qsize 11, out_qsize 3\n",
      "INFO - 19:43:09: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:09: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:09: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:09: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:09: EPOCH - 3 : training on 523625 raw words (199734 effective words) took 1.2s, 167310 effective words/s\n",
      "INFO - 19:43:10: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:10: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:10: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:10: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:10: EPOCH - 4 : training on 523625 raw words (199248 effective words) took 1.0s, 202070 effective words/s\n",
      "INFO - 19:43:11: EPOCH 5 - PROGRESS: at 59.50% examples, 112222 words/s, in_qsize 9, out_qsize 5\n",
      "INFO - 19:43:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:11: EPOCH - 5 : training on 523625 raw words (199389 effective words) took 1.1s, 177243 effective words/s\n",
      "INFO - 19:43:12: EPOCH 6 - PROGRESS: at 70.97% examples, 137865 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 19:43:12: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:12: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:12: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:12: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:13: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:13: EPOCH - 6 : training on 523625 raw words (199404 effective words) took 1.1s, 185593 effective words/s\n",
      "INFO - 19:43:14: EPOCH 7 - PROGRESS: at 82.30% examples, 161027 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 19:43:14: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:14: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:14: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:14: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:14: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:14: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:14: EPOCH - 7 : training on 523625 raw words (199596 effective words) took 1.1s, 189203 effective words/s\n",
      "INFO - 19:43:15: EPOCH 8 - PROGRESS: at 55.57% examples, 109342 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 19:43:15: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:15: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:15: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:15: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:15: EPOCH - 8 : training on 523625 raw words (199622 effective words) took 1.1s, 180122 effective words/s\n",
      "INFO - 19:43:16: EPOCH 9 - PROGRESS: at 80.44% examples, 157296 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 19:43:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:16: EPOCH - 9 : training on 523625 raw words (199548 effective words) took 1.1s, 189439 effective words/s\n",
      "INFO - 19:43:17: EPOCH 10 - PROGRESS: at 80.44% examples, 157863 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 19:43:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:17: EPOCH - 10 : training on 523625 raw words (199210 effective words) took 1.0s, 190657 effective words/s\n",
      "INFO - 19:43:18: EPOCH 11 - PROGRESS: at 90.04% examples, 179530 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 19:43:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:18: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:43:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:18: EPOCH - 11 : training on 523625 raw words (199949 effective words) took 1.0s, 196737 effective words/s\n",
      "INFO - 19:43:19: EPOCH 12 - PROGRESS: at 57.54% examples, 107715 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 19:43:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:19: EPOCH - 12 : training on 523625 raw words (199636 effective words) took 1.1s, 173851 effective words/s\n",
      "INFO - 19:43:20: EPOCH 13 - PROGRESS: at 59.45% examples, 106687 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 19:43:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:20: EPOCH - 13 : training on 523625 raw words (199442 effective words) took 1.2s, 163437 effective words/s\n",
      "INFO - 19:43:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:21: EPOCH - 14 : training on 523625 raw words (199496 effective words) took 1.0s, 204402 effective words/s\n",
      "INFO - 19:43:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:22: EPOCH - 15 : training on 523625 raw words (199710 effective words) took 0.9s, 213147 effective words/s\n",
      "INFO - 19:43:23: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:23: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:23: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:23: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:23: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:23: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:23: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:23: EPOCH - 16 : training on 523625 raw words (199521 effective words) took 0.9s, 221540 effective words/s\n",
      "INFO - 19:43:24: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:24: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:24: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:24: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:24: EPOCH - 17 : training on 523625 raw words (199726 effective words) took 1.0s, 199581 effective words/s\n",
      "INFO - 19:43:25: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:25: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:25: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:25: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:25: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:25: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:25: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:25: EPOCH - 18 : training on 523625 raw words (199751 effective words) took 1.0s, 210010 effective words/s\n",
      "INFO - 19:43:26: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:26: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:26: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:26: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:26: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:26: EPOCH 19 - PROGRESS: at 98.13% examples, 195762 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 19:43:26: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:26: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:26: EPOCH - 19 : training on 523625 raw words (199807 effective words) took 1.0s, 197423 effective words/s\n",
      "INFO - 19:43:27: EPOCH 20 - PROGRESS: at 59.50% examples, 113717 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 19:43:27: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:27: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:27: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:27: EPOCH - 20 : training on 523625 raw words (199324 effective words) took 1.1s, 178248 effective words/s\n",
      "INFO - 19:43:28: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:28: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:28: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:28: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:28: EPOCH - 21 : training on 523625 raw words (199456 effective words) took 0.9s, 221202 effective words/s\n",
      "INFO - 19:43:29: EPOCH 22 - PROGRESS: at 84.24% examples, 167913 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 19:43:29: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:29: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:29: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:29: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:29: EPOCH - 22 : training on 523625 raw words (199872 effective words) took 1.0s, 193043 effective words/s\n",
      "INFO - 19:43:30: EPOCH 23 - PROGRESS: at 76.63% examples, 152077 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 19:43:30: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:30: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:30: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:30: worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:43:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:30: EPOCH - 23 : training on 523625 raw words (199619 effective words) took 1.1s, 190061 effective words/s\n",
      "INFO - 19:43:31: EPOCH 24 - PROGRESS: at 57.54% examples, 114021 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 19:43:31: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:31: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:31: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:31: EPOCH - 24 : training on 523625 raw words (199635 effective words) took 1.1s, 182368 effective words/s\n",
      "INFO - 19:43:32: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:32: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:32: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:32: EPOCH - 25 : training on 523625 raw words (199850 effective words) took 0.9s, 211461 effective words/s\n",
      "INFO - 19:43:33: EPOCH 26 - PROGRESS: at 65.22% examples, 126761 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:34: EPOCH - 26 : training on 523625 raw words (199316 effective words) took 1.1s, 183708 effective words/s\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:34: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:34: EPOCH - 27 : training on 523625 raw words (199389 effective words) took 0.9s, 215754 effective words/s\n",
      "INFO - 19:43:35: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:35: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:35: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:35: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:35: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:35: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:35: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:35: EPOCH - 28 : training on 523625 raw words (200053 effective words) took 0.9s, 234043 effective words/s\n",
      "INFO - 19:43:36: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:36: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:36: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:36: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:36: EPOCH - 29 : training on 523625 raw words (199615 effective words) took 1.0s, 202185 effective words/s\n",
      "INFO - 19:43:37: EPOCH 30 - PROGRESS: at 61.41% examples, 119256 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 19:43:38: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 19:43:38: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 19:43:38: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 19:43:38: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 19:43:38: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 19:43:38: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 19:43:38: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 19:43:38: EPOCH - 30 : training on 523625 raw words (199520 effective words) took 1.1s, 181713 effective words/s\n",
      "INFO - 19:43:38: Word2Vec lifecycle event {'msg': 'training on 15708750 raw words (5987204 effective words) took 31.8s, 188106 effective words/s', 'datetime': '2021-06-24T19:43:38.066805', 'gensim': '4.0.1', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.53 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdb7409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-b8b39c8d9fa1>:3: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 17:25:06: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "### DEPRECATED!!!\n",
    "\n",
    "# As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085f53d",
   "metadata": {},
   "source": [
    "### Exploring the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bcd2ee",
   "metadata": {},
   "source": [
    "Most similar to:\n",
    "\n",
    "Here, we will ask our model to find the word most similar to some of the most iconic characters of the Simpsons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "990529c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.6626412868499756),\n",
       " ('hammock', 0.6492546200752258),\n",
       " ('gosh', 0.649033784866333),\n",
       " ('suspicious', 0.6488982439041138),\n",
       " ('crummy', 0.6417440176010132),\n",
       " ('depressed', 0.6409281492233276),\n",
       " ('glamorous', 0.6363852620124817),\n",
       " ('terrific', 0.6356622576713562),\n",
       " ('bongo', 0.6240394115447998),\n",
       " ('awww', 0.6176309585571289)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fb87bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('recent', 0.6733304858207703),\n",
       " ('sir', 0.6718449592590332),\n",
       " ('easily', 0.6549437046051025),\n",
       " ('congratulation', 0.6546491384506226),\n",
       " ('sherman', 0.6523118019104004),\n",
       " ('select', 0.6451156139373779),\n",
       " ('pleased', 0.6447362899780273),\n",
       " ('hutz', 0.6387559175491333),\n",
       " ('montgomery_burn', 0.6385891437530518),\n",
       " ('kennedy', 0.6298847198486328)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer_simpson\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5026bf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('snuggle', 0.6966519355773926),\n",
       " ('sweetheart', 0.6907212734222412),\n",
       " ('eliza', 0.6902742385864258),\n",
       " ('awww', 0.6883502006530762),\n",
       " ('depressed', 0.6810144782066345),\n",
       " ('crummy', 0.6776387691497803),\n",
       " ('rude', 0.6718450784683228),\n",
       " ('homer', 0.6714166402816772),\n",
       " ('brunch', 0.6688135862350464),\n",
       " ('arrange', 0.6632348299026489)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"marge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ab666ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.7857683300971985),\n",
       " ('homework', 0.7636374235153198),\n",
       " ('convince', 0.7172344923019409),\n",
       " ('assignment', 0.7169288992881775),\n",
       " ('mom_dad', 0.7032962441444397),\n",
       " ('surprised', 0.6915916204452515),\n",
       " ('janey', 0.6913002729415894),\n",
       " ('behave', 0.6891374588012695),\n",
       " ('ralphie', 0.6828575730323792),\n",
       " ('impress', 0.6814612746238708)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44453d54",
   "metadata": {},
   "source": [
    "Similarities:\n",
    "    \n",
    "Here, we will see how similar are two words to each other :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "830860b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65207624"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('maggie', 'baby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e79d1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5326438"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('bart', 'nelson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12b752",
   "metadata": {},
   "source": [
    "Odd-One-Out:\n",
    "    \n",
    "Here, we ask our model to give us the word that does not belong to the list!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ed4e390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nelson'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we compared the friendship between Nelson, Bart, and Milhouse?\n",
    "\n",
    "w2v_model.wv.doesnt_match([\"nelson\", \"bart\", \"milhouse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "182a998a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homer'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last but not least, how is the relationship between Homer and his two sister-in-laws?\n",
    "\n",
    "w2v_model.wv.doesnt_match(['homer', 'patty', 'selma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49589c52",
   "metadata": {},
   "source": [
    "Analogy difference:\n",
    "\n",
    "Which word is to woman as homer is to marge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "175333cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.5862100124359131),\n",
       " ('rude', 0.5613473057746887),\n",
       " ('adopt', 0.5586809515953064)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a945301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.6911569833755493),\n",
       " ('arrange', 0.6468756794929504),\n",
       " ('encourage', 0.6392290592193604)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which word is to woman as bart is to man?\n",
    "\n",
    "w2v_model.wv.most_similar(positive=[\"woman\", \"bart\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06bb0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
