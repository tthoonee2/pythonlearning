{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3\n",
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data and check for missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>Probably my favorite cover! Super sassy and ve...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>This case protects the phone from damage.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>Nice</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>this was another of my favorite ones, thanks f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>Decent case but not a lot of protection.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>it is so easy to put on your phone and it prot...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>Much better quality than I expected for the pr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>This is one of the best screen protectors I ha...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>This kit included a microfiber cloth and soft ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>This was the easiest screen protector to put o...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin                                         reviewText  overall\n",
       "0      B007D6J64K  Probably my favorite cover! Super sassy and ve...        5\n",
       "1      B007D6J64K          This case protects the phone from damage.        5\n",
       "2      B007D6J64K                                               Nice        4\n",
       "3      B007D6J64K  this was another of my favorite ones, thanks f...        5\n",
       "4      B007D6J64K           Decent case but not a lot of protection.        5\n",
       "...           ...                                                ...      ...\n",
       "29995  B0096QI0QK  it is so easy to put on your phone and it prot...        5\n",
       "29996  B0096QI0QK  Much better quality than I expected for the pr...        5\n",
       "29997  B0096QI0QK  This is one of the best screen protectors I ha...        4\n",
       "29998  B0096QI0QK  This kit included a microfiber cloth and soft ...        5\n",
       "29999  B0096QI0QK  This was the easiest screen protector to put o...        5\n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"amazon_cellphones_multiclass.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   asin        30000 non-null  object\n",
      " 1   reviewText  29988 non-null  object\n",
      " 2   overall     30000 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 703.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin           0\n",
       "reviewText    12\n",
       "overall        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the binary target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    17695\n",
       "4     5366\n",
       "3     3144\n",
       "1     2121\n",
       "2     1674\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable overall distribution\n",
    "\n",
    "df.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to binarize overall\n",
    "\n",
    "def binary(row):\n",
    "    if row['overall'] > 3:\n",
    "        val = 1\n",
    "    elif row['overall'] < 3:\n",
    "        val = 0\n",
    "    else:\n",
    "        val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>bin_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>Probably my favorite cover! Super sassy and ve...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>This case protects the phone from damage.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>Nice</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>this was another of my favorite ones, thanks f...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B007D6J64K</td>\n",
       "      <td>Decent case but not a lot of protection.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>it is so easy to put on your phone and it prot...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>Much better quality than I expected for the pr...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>This is one of the best screen protectors I ha...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>This kit included a microfiber cloth and soft ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>B0096QI0QK</td>\n",
       "      <td>This was the easiest screen protector to put o...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             asin                                         reviewText  overall  \\\n",
       "0      B007D6J64K  Probably my favorite cover! Super sassy and ve...        5   \n",
       "1      B007D6J64K          This case protects the phone from damage.        5   \n",
       "2      B007D6J64K                                               Nice        4   \n",
       "3      B007D6J64K  this was another of my favorite ones, thanks f...        5   \n",
       "4      B007D6J64K           Decent case but not a lot of protection.        5   \n",
       "...           ...                                                ...      ...   \n",
       "29995  B0096QI0QK  it is so easy to put on your phone and it prot...        5   \n",
       "29996  B0096QI0QK  Much better quality than I expected for the pr...        5   \n",
       "29997  B0096QI0QK  This is one of the best screen protectors I ha...        4   \n",
       "29998  B0096QI0QK  This kit included a microfiber cloth and soft ...        5   \n",
       "29999  B0096QI0QK  This was the easiest screen protector to put o...        5   \n",
       "\n",
       "       bin_y  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "29995      1  \n",
       "29996      1  \n",
       "29997      1  \n",
       "29998      1  \n",
       "29999      1  \n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding the new variable to the dataset\n",
    "\n",
    "df['bin_y'] = df.apply(binary, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all NaN and split in explicative and dependent: X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_na = df[~(df['reviewText'].isna()) & ~(df['bin_y']==-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_0 = df_not_na['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Probably my favorite cover! Super sassy and ve...\n",
       "1            This case protects the phone from damage.\n",
       "2                                                 Nice\n",
       "3    this was another of my favorite ones, thanks f...\n",
       "4             Decent case but not a lot of protection.\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_0[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This case is so cute the only problem I had with it due to the texture of the case it was hard to get in and out of my pockets'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_0[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_not_na['bin_y'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing: lowercase, remove punctuation, tokenize, lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable), and you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "<ipython-input-13-fbbefc4c9292>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text_0.str.lower().str.replace('[^\\w\\s]',' ') # RegEx = regular expression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'case', 'is', 'so', 'cute', 'the', 'only', 'problem', 'i', 'had', 'with', 'it', 'due', 'to', 'the', 'texture', 'of', 'the', 'case', 'it', 'wa', 'hard', 'to', 'get', 'in', 'and', 'out', 'of', 'my', 'pocket']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = text_0.str.lower().str.replace('[^\\w\\s]',' ') # RegEx = regular expression\n",
    "\n",
    "text = text.str.split()\n",
    "\n",
    "# text = text.apply(lambda x: [lemmatizer.lemmatize(word) for sentence in x for word in sentence])\n",
    "text = text.apply(lambda x: [lemmatizer.lemmatize(sent) for sent in x])\n",
    "\n",
    "print(text[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lambda functions:   https://www.w3schools.com/python/python_lambda.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(n):\n",
    "  return lambda a : a * n\n",
    "\n",
    "mydoubler = myfunc(2)\n",
    "mytripler = myfunc(3)\n",
    "\n",
    "print(mydoubler(11))\n",
    "print(mytripler(11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use lambda functions when an anonymous function is required for a short period of time, inside another function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List Comprehension: https://www.w3schools.com/python/python_lists_comprehension.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\n",
    "\n",
    "newlist = []\n",
    "for x in fruits:\n",
    "  if \"a\" in x:\n",
    "    newlist.append(x)\n",
    "\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\n",
    "\n",
    "newlist = [x for x in fruits if \"a\" in x]\n",
    "\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stopwords can be found at [this link](https://gist.github.com/sebleier/554280), downloaded, custiomized and imported as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a new library: gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not using Anaconda:\n",
    "\n",
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "Requirement already satisfied: setuptools in w:\\programmi\\anaconda\\lib\\site-packages (from python-Levenshtein) (49.6.0.post20210108)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py): started\n",
      "  Building wheel for python-Levenshtein (setup.py): finished with status 'done'\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp38-cp38-win_amd64.whl size=83098 sha256=741b4962eba7fcc5f9afae915dce0b0b0cbe14b791c0981fd28a87489ba387b1\n",
      "  Stored in directory: c:\\users\\utente\\appdata\\local\\pip\\cache\\wheels\\d7\\0c\\76\\042b46eb0df65c3ccd0338f791210c55ab79d209bcc269e2c7\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Utente/nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utente\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mW:\\Programmi\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Programmi\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Utente/nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utente\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8a66f1d5e064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'good'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'many'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'love'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'excellent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'would'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Programmi\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Programmi\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Programmi\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Programmi\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Utente/nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'W:\\\\Programmi\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Utente\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(['good', 'many', 'love', 'excellent', 'would'])\n",
    "\n",
    "bigram = Phrases(text, min_count=5, threshold=0.2, connector_words=stop)\n",
    "print(bigram[text[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Phrases in module gensim.models.phrases:\n",
      "\n",
      "class Phrases(_PhrasesTransformation)\n",
      " |  Phrases(sentences=None, min_count=5, threshold=10.0, max_vocab_size=40000000, delimiter='_', progress_per=10000, scoring='default', connector_words=frozenset())\n",
      " |  \n",
      " |  Detect phrases based on collocation counts.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Phrases\n",
      " |      _PhrasesTransformation\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, min_count=5, threshold=10.0, max_vocab_size=40000000, delimiter='_', progress_per=10000, scoring='default', connector_words=frozenset())\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          The `sentences` iterable can be simply a list, but for larger corpora, consider a generator that streams\n",
      " |          the sentences directly from disk/network, See :class:`~gensim.models.word2vec.BrownCorpus`,\n",
      " |          :class:`~gensim.models.word2vec.Text8Corpus` or :class:`~gensim.models.word2vec.LineSentence`\n",
      " |          for such examples.\n",
      " |      min_count : float, optional\n",
      " |          Ignore all words and bigrams with total collected count lower than this value.\n",
      " |      threshold : float, optional\n",
      " |          Represent a score threshold for forming the phrases (higher means fewer phrases).\n",
      " |          A phrase of words `a` followed by `b` is accepted if the score of the phrase is greater than threshold.\n",
      " |          Heavily depends on concrete scoring-function, see the `scoring` parameter.\n",
      " |      max_vocab_size : int, optional\n",
      " |          Maximum size (number of tokens) of the vocabulary. Used to control pruning of less common words,\n",
      " |          to keep memory under control. The default of 40M needs about 3.6GB of RAM. Increase/decrease\n",
      " |          `max_vocab_size` depending on how much available memory you have.\n",
      " |      delimiter : str, optional\n",
      " |          Glue character used to join collocation tokens.\n",
      " |      scoring : {'default', 'npmi', function}, optional\n",
      " |          Specify how potential phrases are scored. `scoring` can be set with either a string that refers to a\n",
      " |          built-in scoring function, or with a function with the expected parameter names.\n",
      " |          Two built-in scoring functions are available by setting `scoring` to a string:\n",
      " |      \n",
      " |          #. \"default\" - :func:`~gensim.models.phrases.original_scorer`.\n",
      " |          #. \"npmi\" - :func:`~gensim.models.phrases.npmi_scorer`.\n",
      " |      connector_words : set of str, optional\n",
      " |          Set of words that may be included within a phrase, without affecting its scoring.\n",
      " |          No phrase can start nor end with a connector word; a phrase may contain any number of\n",
      " |          connector words in the middle.\n",
      " |      \n",
      " |          **If your texts are in English, set** ``connector_words=phrases.ENGLISH_CONNECTOR_WORDS``.\n",
      " |      \n",
      " |          This will cause phrases to include common English articles, prepositions and\n",
      " |          conjuctions, such as `bank_of_america` or `eye_of_the_beholder`.\n",
      " |      \n",
      " |          For other languages or specific applications domains, use custom ``connector_words``\n",
      " |          that make sense there: ``connector_words=frozenset(\"der die das\".split())`` etc.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models.word2vec import Text8Corpus\n",
      " |          >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
      " |          >>>\n",
      " |          >>> # Load corpus and train a model.\n",
      " |          >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |          >>> phrases = Phrases(sentences, min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
      " |          >>>\n",
      " |          >>> # Use the model to detect phrases in a new sentence.\n",
      " |          >>> sent = [u'trees', u'graph', u'minors']\n",
      " |          >>> print(phrases[sent])\n",
      " |          [u'trees_graph', u'minors']\n",
      " |          >>>\n",
      " |          >>> # Or transform multiple sentences at once.\n",
      " |          >>> sents = [[u'trees', u'graph', u'minors'], [u'graph', u'minors']]\n",
      " |          >>> for phrase in phrases[sents]:\n",
      " |          ...     print(phrase)\n",
      " |          [u'trees_graph', u'minors']\n",
      " |          [u'graph_minors']\n",
      " |          >>>\n",
      " |          >>> # Export a FrozenPhrases object that is more efficient but doesn't allow any more training.\n",
      " |          >>> frozen_phrases = phrases.freeze()\n",
      " |          >>> print(frozen_phrases[sent])\n",
      " |          [u'trees_graph', u'minors']\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      The ``scoring=\"npmi\"`` is more robust when dealing with common words that form part of common bigrams, and\n",
      " |      ranges from -1 to 1, but is slower to calculate than the default ``scoring=\"default\"``.\n",
      " |      The default is the PMI-like scoring as described in `Mikolov, et. al: \"Distributed\n",
      " |      Representations of Words and Phrases and their Compositionality\" <https://arxiv.org/abs/1310.4546>`_.\n",
      " |      \n",
      " |      To use your own custom ``scoring`` function, pass in a function with the following signature:\n",
      " |      \n",
      " |      * ``worda_count`` - number of corpus occurrences in `sentences` of the first token in the bigram being scored\n",
      " |      * ``wordb_count`` - number of corpus occurrences in `sentences` of the second token in the bigram being scored\n",
      " |      * ``bigram_count`` - number of occurrences in `sentences` of the whole bigram\n",
      " |      * ``len_vocab`` - the number of unique tokens in `sentences`\n",
      " |      * ``min_count`` - the `min_count` setting of the Phrases class\n",
      " |      * ``corpus_word_count`` - the total number of tokens (non-unique) in `sentences`\n",
      " |      \n",
      " |      The scoring function must accept all these parameters, even if it doesn't use them in its scoring.\n",
      " |      \n",
      " |      The scoring function **must be pickleable**.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_vocab(self, sentences)\n",
      " |      Update model parameters with new `sentences`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Text corpus to update this model's parameters from.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models.word2vec import Text8Corpus\n",
      " |          >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
      " |          >>>\n",
      " |          >>> # Train a phrase detector from a text corpus.\n",
      " |          >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |          >>> phrases = Phrases(sentences, connector_words=ENGLISH_CONNECTOR_WORDS)  # train model\n",
      " |          >>> assert len(phrases.vocab) == 37\n",
      " |          >>>\n",
      " |          >>> more_sentences = [\n",
      " |          ...     [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there'],\n",
      " |          ...     [u'machine', u'learning', u'can', u'be', u'new', u'york', u'sometimes'],\n",
      " |          ... ]\n",
      " |          >>>\n",
      " |          >>> phrases.add_vocab(more_sentences)  # add new sentences to model\n",
      " |          >>> assert len(phrases.vocab) == 60\n",
      " |  \n",
      " |  export_phrases(self)\n",
      " |      Extract all found phrases.\n",
      " |      \n",
      " |      Returns\n",
      " |      ------\n",
      " |      dict(str, float)\n",
      " |          Mapping between phrases and their scores.\n",
      " |  \n",
      " |  freeze(self)\n",
      " |      Return an object that contains the bare minimum of information while still allowing\n",
      " |      phrase detection. See :class:`~gensim.models.phrases.FrozenPhrases`.\n",
      " |      \n",
      " |      Use this \"frozen model\" to dramatically reduce RAM footprint if you don't plan to\n",
      " |      make any further changes to your `Phrases` model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.phrases.FrozenPhrases`\n",
      " |          Exported object that's smaller, faster, but doesn't support model updates.\n",
      " |  \n",
      " |  score_candidate(self, word_a, word_b, in_between)\n",
      " |      Score a single phrase candidate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      (str, float)\n",
      " |          2-tuple of ``(delimiter-joined phrase, phrase score)`` for a phrase,\n",
      " |          or ``(None, None)`` if not a phrase.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _PhrasesTransformation:\n",
      " |  \n",
      " |  __getitem__(self, sentence)\n",
      " |      Convert the input sequence of tokens ``sentence`` into a sequence of tokens where adjacent\n",
      " |              tokens are replaced by a single token if they form a bigram collocation.\n",
      " |      \n",
      " |              If `sentence` is an entire corpus (iterable of sentences rather than a single\n",
      " |              sentence), return an iterable that converts each of the corpus' sentences\n",
      " |              into phrases on the fly, one after another.\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              sentence : {list of str, iterable of list of str}\n",
      " |                  Input sentence or a stream of sentences.\n",
      " |      \n",
      " |              Return\n",
      " |              ------\n",
      " |              {list of str, iterable of list of str}\n",
      " |                  Sentence with phrase tokens joined by ``self.delimiter``, if input was a single sentence.\n",
      " |                  A generator of such sentences if input was a corpus.\n",
      " |      \n",
      " |      s\n",
      " |  \n",
      " |  analyze_sentence(self, sentence)\n",
      " |      Analyze a sentence, concatenating any detected phrases into a single token.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : iterable of str\n",
      " |          Token sequence representing the sentence to be analyzed.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      (str, {float, None})\n",
      " |          Iterate through the input sentence tokens and yield 2-tuples of:\n",
      " |          - ``(concatenated_phrase_tokens, score)`` for token sequences that form a phrase.\n",
      " |          - ``(word, None)`` if the token is not a part of a phrase.\n",
      " |  \n",
      " |  find_phrases(self, sentences)\n",
      " |      Get all unique phrases (multi-word expressions) that appear in ``sentences``, and their scores.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Text corpus.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict(str, float)\n",
      " |         Unique phrases found in ``sentences``, mapped to their scores.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>> from gensim.models.word2vec import Text8Corpus\n",
      " |          >>> from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
      " |          >>>\n",
      " |          >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |          >>> phrases = Phrases(sentences, min_count=1, threshold=0.1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
      " |          >>>\n",
      " |          >>> for phrase, score in phrases.find_phrases(sentences).items():\n",
      " |          ...     print(phrase, score)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from _PhrasesTransformation:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.phrases.Phrases` /\n",
      " |      :class:`~gensim.models.phrases.FrozenPhrases` model.\n",
      " |      \n",
      " |      Handles backwards compatibility from older versions which did not support pluggable scoring functions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : object\n",
      " |          See :class:`~gensim.utils.SaveLoad.load`.\n",
      " |      kwargs : object\n",
      " |          See :class:`~gensim.utils.SaveLoad.load`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      " |      Save the object to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      " |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      " |          loading and sharing the large arrays in RAM between multiple processes.\n",
      " |      \n",
      " |          If list of str: store these attributes into separate files. The automated size check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int, optional\n",
      " |          Don't store arrays smaller than this separately. In bytes.\n",
      " |      ignore : frozenset of str, optional\n",
      " |          Attributes that shouldn't be stored at all.\n",
      " |      pickle_protocol : int, optional\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |          Load object from file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold parameter:\n",
    "<img src='img/phrases_threshold.PNG' width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1e13bb0a53a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-1e13bb0a53a6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bigram' is not defined"
     ]
    }
   ],
   "source": [
    "bigrams = [bigram[item] for item in text]\n",
    "ngrams = [bigram[item] for item in bigrams]\n",
    "print(ngrams[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['probably', 'my', 'favorite', 'cover', 'super', 'sassy', 'and', 'very', 'protective', 'i', 'am', 'very', 'abusive', 'of', 'my', 'phone', 'and', 'this', 'case', 'held_up_very_well', 'after', 'a', 'year', 'the', 'color', 'started', 'to', 'wear', 'a', 'bit', 'but', 'it', 'continued', 'to', 'protect_my_phone', 'very', 'well', 'i', 'would', 'buy', 'it', 'again']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'case cute problem due texture_of_the_case wa_hard get pocket'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(['good', 'bad', 'dont', 'many', 'love', 'excellent', 'would', 'perfect', 'even', 'great'])\n",
    "\n",
    "print(ngrams[0])\n",
    "train_sentences = []\n",
    "for row in ngrams:\n",
    "    train_sentences.append(' '.join([item for item in row if item not in stop]))\n",
    "# train_sentences = [' '.join(item) for item in ngrams]\n",
    "train_sentences[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-68415c32931d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_not_na['reviewText'] = train_sentences\n"
     ]
    }
   ],
   "source": [
    "df_not_na['reviewText'] = train_sentences\n",
    "df_not_na.to_csv('amazon_cellphones_binary.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words:\n",
    "<img src='img/bow.PNG' width='600'>\n",
    "\n",
    "Term frequency - inverse document frequency:\n",
    "<img src='img/tfidf.jpeg' width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=1000)\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=1000)\n",
    "\n",
    "X = vectorizer.fit_transform(train_sentences)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "X = X.toarray()\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  [86.90932311621967, 86.41443167305236, 86.66773111927192]\n",
      "recall:  [0.8769637836943939, 0.872245971719829, 0.8746495134421903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "model = tree.DecisionTreeClassifier(max_leaf_nodes=10, max_depth=5)\n",
    "# model = LogisticRegression(class_weight=None)\n",
    "# model = RandomForestClassifier()\n",
    "\n",
    "cvscores = []\n",
    "cvrecall = []\n",
    "\n",
    "for train, test in tqdm(kfold.split(x_train, y_train)):\n",
    "    model.fit(x_train[train],y_train[train])\n",
    "    predicted = model.predict(x_train[test])\n",
    "    scores = accuracy_score(predicted, y_train[test])\n",
    "    recall = recall_score(predicted, y_train[test])\n",
    "    cvrecall.append(recall)\n",
    "    cvscores.append(scores * 100)\n",
    "\n",
    "print(\"accuracy: \",cvscores)\n",
    "print(\"recall: \",cvrecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.12      0.20      1138\n",
      "           1       0.87      0.99      0.93      6916\n",
      "\n",
      "    accuracy                           0.87      8054\n",
      "   macro avg       0.75      0.55      0.56      8054\n",
      "weighted avg       0.84      0.87      0.82      8054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "predicted = model.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validated grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Wall time: 8min 39s\n",
      "Best: 0.856129 using {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.839784 (0.005370) with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "0.844289 (0.002538) with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.837271 (0.002848) with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "0.851750 (0.000699) with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.835758 (0.000493) with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "0.849854 (0.001796) with: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 20, 'n_estimators': 100}\n",
      "0.805314 (0.024144) with: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "0.825892 (0.008594) with: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.792501 (0.005277) with: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "0.825981 (0.006194) with: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.810517 (0.008519) with: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "0.824695 (0.004084) with: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 20, 'n_estimators': 100}\n",
      "0.809384 (0.009318) with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "0.838753 (0.003259) with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.804729 (0.008766) with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "0.835823 (0.003582) with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.811564 (0.013293) with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "0.832214 (0.005765) with: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 20, 'n_estimators': 100}\n",
      "0.845627 (0.002975) with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "0.849371 (0.006014) with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.849235 (0.002599) with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "0.856129 (0.002818) with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.842279 (0.002441) with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "0.856019 (0.003113) with: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20, 'n_estimators': 100}\n",
      "0.802003 (0.007354) with: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "0.829760 (0.008756) with: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.810323 (0.003904) with: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "0.829281 (0.013417) with: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.798120 (0.011425) with: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "0.828597 (0.004230) with: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 20, 'n_estimators': 100}\n",
      "0.817993 (0.005572) with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "0.835676 (0.005106) with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.819389 (0.010369) with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "0.839268 (0.003538) with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "0.817852 (0.002495) with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20, 'n_estimators': 10}\n",
      "0.834378 (0.004844) with: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 20, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "#cross_validated_grid_search for Random Forest\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "param_grid = {'n_estimators': [10, 100],\n",
    "               'criterion': ['gini', 'entropy'],\n",
    "               'max_depth': [None, 5, 10],\n",
    "               'min_samples_split': [2, 10, 20]}\n",
    "\n",
    "#cross_validated_grid_search for SVC\n",
    "# model = svm.SVC()\n",
    "# param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, cv=3, verbose=2, n_jobs=-1, scoring='f1_weighted')\n",
    "# Fit the random search model\n",
    "%time grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "#print grid search results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.40      0.46      1138\n",
      "           1       0.91      0.94      0.92      6916\n",
      "\n",
      "    accuracy                           0.87      8054\n",
      "   macro avg       0.73      0.67      0.69      8054\n",
      "weighted avg       0.85      0.87      0.86      8054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = grid.best_estimator_\n",
    "predicted = best_model.predict(x_test)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
